{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import litellm\n",
    "\n",
    "from litellm import experimental_mcp_client\n",
    "from mcp import ClientSession, StdioServerParameters\n",
    "from mcp.client.stdio import stdio_client\n",
    "from mcp.types import CallToolResult"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MCP TOOLS:  [{'type': 'function', 'function': {'name': 'query_data', 'description': 'Execute SQL queries safely', 'parameters': {'properties': {'sql': {'title': 'Sql', 'type': 'string'}}, 'required': ['sql'], 'title': 'query_dataArguments', 'type': 'object'}, 'strict': False}}]\n",
      "LLM RESPONSE:  \"ModelResponse(id='chatcmpl-e043c9be-f650-48d0-b5ed-ae767e2f7874', created=1744683841, model='ollama_chat/llama3.2:latest', object='chat.completion', system_fingerprint=None, choices=[Choices(finish_reason='stop', index=0, message=Message(content='', role='assistant', tool_calls=[ChatCompletionMessageToolCall(function=Function(arguments='{\\\"sql\\\": \\\"SELECT T1.Name FROM artists AS T1 INNER JOIN albums AS T2 ON T1.ArtistId = T2.ArtistId GROUP BY T1.Name ORDER BY COUNT(T2.AlbumId) DESC LIMIT 3\\\"}', name='query_data'), id='260cf13e-a431-4cf9-90c3-a6259f3f15c3', type='function')], function_call=None, provider_specific_fields=None))], usage=Usage(completion_tokens=61, prompt_tokens=1292, total_tokens=1353, completion_tokens_details=None, prompt_tokens_details=None))\"\n",
      "TOOL_RESULT:  meta=None content=[TextContent(type='text', text=\"('Iron Maiden',)\\n('Led Zeppelin',)\\n('Deep Purple',)\", annotations=None)] isError=False\n",
      "('Iron Maiden',)\n",
      "('Led Zeppelin',)\n",
      "('Deep Purple',)\n",
      "LLM RESPONSE:  \"ModelResponse(id='chatcmpl-38837b39-f75b-424d-b1ad-395ad0b8b907', created=1744683841, model='ollama_chat/llama3.2:latest', object='chat.completion', system_fingerprint=None, choices=[Choices(finish_reason='stop', index=0, message=Message(content='Based on the tool output, here is the answer to the original user question:\\\\n\\\\nThe top three artists by record sales are:\\\\n\\\\n1. Iron Maiden\\\\n2. Led Zeppelin\\\\n3. Deep Purple', role='assistant', tool_calls=None, function_call=None, provider_specific_fields=None))], usage=Usage(completion_tokens=41, prompt_tokens=104, total_tokens=145, completion_tokens_details=None, prompt_tokens_details=None))\"\n",
      "Based on the tool output, here is the answer to the original user question:\n",
      "\n",
      "The top three artists by record sales are:\n",
      "\n",
      "1. Iron Maiden\n",
      "2. Led Zeppelin\n",
      "3. Deep Purple\n"
     ]
    }
   ],
   "source": [
    "model = 'ollama_chat/llama3.2:latest'\n",
    "\n",
    "server_params = StdioServerParameters(\n",
    "    command=\"python3\",\n",
    "    args=[\"./main.py\"],\n",
    ")\n",
    "\n",
    "async with stdio_client(server_params) as (read, write):\n",
    "    async with ClientSession(read, write) as session:\n",
    "        # Initialize the connection\n",
    "        await session.initialize()\n",
    "\n",
    "        meta, contents = await session.read_resource(\"schema://main\")\n",
    "\n",
    "        messages = []\n",
    "        for content in contents[1]:\n",
    "            messages = [{\"role\": \"system\", \"content\": f\"You are a helpful assistant. Your job is to accept natural language queries and return the SQL query that answers the question. You must only return well formed SQL for sqlite3. You may only use select statments as this is for reporting only. Here is the schema of the database:{content.text}\"}]\n",
    "\n",
    "        # Get tools\n",
    "        tools = await experimental_mcp_client.load_mcp_tools(session=session, format=\"openai\")\n",
    "        print(\"MCP TOOLS: \", tools)\n",
    "         \n",
    "         \n",
    "        messages.append({\"role\": \"user\", \"content\": \"List the top three artitsts by record sales\"})\n",
    "        llm_response = await litellm.acompletion(\n",
    "            model=model,\n",
    "            messages=messages,\n",
    "            tools=tools,\n",
    "            temperature=0.0,\n",
    "        )\n",
    "        print(\"LLM RESPONSE: \", json.dumps(llm_response, indent=4, default=str))\n",
    "        \n",
    "        fn: str = llm_response[\"choices\"][0][\"message\"][\"tool_calls\"][0].function.name\n",
    "        args: dict = json.loads(llm_response[\"choices\"][0][\"message\"][\"tool_calls\"][0].function.arguments)\n",
    "\n",
    "        messages.append({\"role\": \"assistant\", \"tool_calls\": json.dumps([{\"type\": \"function\", \"function\": {\"name\": fn, \"arguments\": args}}])})\n",
    "\n",
    "        tool_result: CallToolResult = await session.call_tool(fn, args)\n",
    "        messages.append({\"role\": \"tool\", \"content\": tool_result.content[0].text})\n",
    "\n",
    "        print(\"TOOL_RESULT: \", tool_result)\n",
    "\n",
    "        print(tool_result.content[0].text)\n",
    "\n",
    "        messages[0][\"content\"] = \"You are a helpful assistant that can answer questions about the database.\"\n",
    "\n",
    "        llm_response = await litellm.acompletion(\n",
    "            model=model,\n",
    "            messages=messages,\n",
    "            tools=tools,\n",
    "            temperature=0.0,\n",
    "        )\n",
    "\n",
    "        print(\"LLM RESPONSE: \", json.dumps(llm_response, indent=4, default=str))\n",
    "\n",
    "\n",
    "        print(llm_response[\"choices\"][0][\"message\"]['content'])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
